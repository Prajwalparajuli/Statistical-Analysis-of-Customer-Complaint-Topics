{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2723640e",
   "metadata": {},
   "source": [
    "### Loading the preprocessed complaints data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5dde971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DataFrame loaded successfully.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 164003 entries, 1978 to 11013727\n",
      "Data columns (total 4 columns):\n",
      " #   Column                        Non-Null Count   Dtype \n",
      "---  ------                        --------------   ----- \n",
      " 0   Product                       164003 non-null  object\n",
      " 1   Consumer complaint narrative  164003 non-null  object\n",
      " 2   Consumer disputed?            164003 non-null  object\n",
      " 3   cleaned_tokens                164003 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 6.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Caliber Home Loans has engaged in the prohibit...</td>\n",
       "      <td>No</td>\n",
       "      <td>[caliber, home, loan, engaged, prohibited, pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>I have filed numerous complaints in an attempt...</td>\n",
       "      <td>No</td>\n",
       "      <td>[filed, numerous, complaint, attempt, stop, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2177</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>To Whom it may concern : Consumer Collection M...</td>\n",
       "      <td>No</td>\n",
       "      <td>[may, concern, consumer, collection, managemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>Credit card</td>\n",
       "      <td>I received a letter dated XXXX/XXXX/15 stating...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[received, letter, dated, xxxx/xxxx/15, statin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>In 2011 I purchase a new phone at a XXXX store...</td>\n",
       "      <td>No</td>\n",
       "      <td>[2011, purchase, new, phone, xxxx, store, xxxx...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Product                       Consumer complaint narrative  \\\n",
       "1978         Mortgage  Caliber Home Loans has engaged in the prohibit...   \n",
       "2077         Mortgage  I have filed numerous complaints in an attempt...   \n",
       "2177  Debt collection  To Whom it may concern : Consumer Collection M...   \n",
       "2231      Credit card  I received a letter dated XXXX/XXXX/15 stating...   \n",
       "2500  Debt collection  In 2011 I purchase a new phone at a XXXX store...   \n",
       "\n",
       "     Consumer disputed?                                     cleaned_tokens  \n",
       "1978                 No  [caliber, home, loan, engaged, prohibited, pat...  \n",
       "2077                 No  [filed, numerous, complaint, attempt, stop, na...  \n",
       "2177                 No  [may, concern, consumer, collection, managemen...  \n",
       "2231                Yes  [received, letter, dated, xxxx/xxxx/15, statin...  \n",
       "2500                 No  [2011, purchase, new, phone, xxxx, store, xxxx...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load the Pre-processed Data ---\n",
    "import pandas as pd\n",
    "\n",
    "processed_file_path = \"processed_complaints.pkl\"\n",
    "complaints_df = pd.read_pickle(processed_file_path)\n",
    "\n",
    "print(\"Processed DataFrame loaded successfully.\")\n",
    "complaints_df.info()\n",
    "complaints_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08020fb1",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "Vectorization is the process of converting unstructured text data into a numerical format (vectors) that machine learning models can understand and process. The transformation enables models to perform various NLP task, such as text classification and semantic analysis, by capturing the relationships and meaning of words and documents.\n",
    "#### Vectorization Technique \n",
    "Bag of Words (BoW):\n",
    "A bag-of-words (BoW) (corpus) model is simplified represntation of text that lists the frequency of words in a document while disregarding theri original order, context, and syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7f0ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the dictionary after filtering: 22357\n",
      "Number of documents in the corpus: 164003\n",
      "\n",
      "Example of the first document in the corpus (word_id, word_frequency):\n",
      "[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 2), (6, 2), (7, 16), (8, 1), (9, 5), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 4), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27, 3), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 2), (43, 1), (44, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 3), (52, 1), (53, 1), (54, 1), (55, 1), (56, 2), (57, 4), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 3), (67, 1), (68, 1), (69, 8), (70, 1), (71, 1), (72, 4), (73, 1), (74, 1), (75, 6), (76, 1), (77, 1), (78, 4), (79, 2), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 5), (88, 1), (89, 1), (90, 1), (91, 3), (92, 1), (93, 1), (94, 1), (95, 1), (96, 3), (97, 1), (98, 1), (99, 1), (100, 2), (101, 2), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 2), (118, 1), (119, 3), (120, 6), (121, 1), (122, 2), (123, 2), (124, 1), (125, 1), (126, 1), (127, 2), (128, 2), (129, 2), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 5), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 2), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 3)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Our text data is the 'cleaned_tokens' column\n",
    "documents = complaints_df['cleaned_tokens']\n",
    "\n",
    "# Create a mapping from word to integer ID\n",
    "dictionary = Dictionary(documents)\n",
    "\n",
    "# Filter out words that are too rare or too common.\n",
    "# no_below = 5: Ignore words that appear in less than 5 documents\n",
    "# no_above = 0.5: Ignore words that appear in more than 50% of all documents\n",
    "dictionary.filter_extremes(no_below = 5, no_above = 0.5)\n",
    "\n",
    "# Create a bag-of-words representation for each document\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# --- Verification ---\n",
    "print(f\"Number of unique words in the dictionary after filtering: {len(dictionary)}\")\n",
    "print(f\"Number of documents in the corpus: {len(corpus)}\")\n",
    "print(\"\\nExample of the first document in the corpus (word_id, word_frequency):\")\n",
    "# This shows the numeric representation of the first complaint narrative\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8349a1",
   "metadata": {},
   "source": [
    "#### Term Weighting with TF-IDF\n",
    "While BoW is effective in creating vector representation of words with their frequency, it gives equal importance to all terms based on their frequency. A more nuanced representation is TF-IDF (Term Frequency-Inverse Document Frequency) which re-weights the term counts to emphasize terms that are frequent within a document but rare across the entire corpus, thus highlighting terms that are more distinctive to that documents's content. \n",
    "- Term Frequency (TF): Measures the frequency of a term $t$ in a document $d$.\n",
    "- Inverse Document Frequency (IDF): Measures the rarity of a term $t$ across the corpus $C$. $IDF(t, C) = log(N / df_t)$ where $N$ is the total number of documents and $df_t$ is the number of documents containing term $t$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "940576cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF model from the corpus...\n",
      "TF-IDF transformation complete.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "# --- Step 1: TF-IDF Transformation ---\n",
    "print(\"Creating TF-IDF model from the corpus...\")\n",
    "\n",
    "# The TfidfModel is trained on the raw count corpus\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# The trained model is then used to transform the entire corpus\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "print(\"TF-IDF transformation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17174fc4",
   "metadata": {},
   "source": [
    "#### Latent Dirichlent Allocation (LDA)\n",
    "LDA is a generative probabilistic topic models which assumes that each document is a finite mixture over a set of latent topics, and each topic is a  probability distribution over a vocabulary of terms. The algorithm's objective is to reverse-engineer this generative process. Given the observed documents (our corpus), LDA computes the posterior distributions for:\n",
    "\n",
    "- The per-document topic distributions (often denoted as θ).\n",
    "- The per-topic word distributions (often denoted as φ). This is achieved through algorithms like Gibbs sampling or Variational Bayes.\n",
    "\n",
    "In simpler term, LDA assumes a document can be a mixture of multiple topics, not just one. Taking an example of song playlist, when categorizing a song in the playlist. LDA understands that a song can be a mix of genres and it might analyze a song and decide as 70% Rock, 20% blues and 10% folk, which are probabilistic assignment to the song. LDA does this for all the songs and creates numbber of topics (k) as the probability outcomes (7 different genres of song). After LDA give the probabistic outcomes for each song with the 7 genres, we will pick the topic with highest probability as the main genre of that song.\n",
    "\n",
    "The hyperparameter for the LDA is number of topics (k) where the optimal number of topic is necessary to find which is the most important steps in topic modeling: \n",
    "- Too few topics: If 'k' is too small, the topics become overly broad and mix together distinct concepts, making them vague and not very useful. For example, a single \"Loan issues\" topic might incorrectly group together very different complaints about mortgages, student loans, and auto loans. \n",
    "- Too many topics: If 'k' is too large, the topics become too granular and often overlap significantly. This creates redundant topcis that are difficult to interpret and don't provide a clear, high-level view of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde15ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LDA model with 7 topics...\n",
      "LDA model training complete.\n",
      "\n",
      "Discovered Topics (word distributions):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"loan\" + 0.008*\"payment\" + 0.008*\"mortgage\" + 0.004*\"xx/xx/xxxx\" + 0.004*\"modification\" + 0.004*\"home\" + 0.004*\"``\" + 0.004*\"interest\" + 0.004*\"month\" + 0.003*\"year\"'),\n",
       " (1,\n",
       "  '0.019*\"car\" + 0.014*\"vehicle\" + 0.009*\"lease\" + 0.006*\"santander\" + 0.005*\"sps\" + 0.005*\"lien\" + 0.005*\"apartment\" + 0.005*\"dealership\" + 0.004*\"ally\" + 0.004*\"bayview\"'),\n",
       " (2,\n",
       "  '0.015*\"report\" + 0.013*\"credit\" + 0.010*\"reporting\" + 0.010*\"debt\" + 0.010*\"account\" + 0.009*\"equifax\" + 0.008*\"information\" + 0.007*\"collection\" + 0.007*\"experian\" + 0.007*\"inquiry\"'),\n",
       " (3,\n",
       "  '0.012*\"bonus\" + 0.009*\"macy\" + 0.008*\"promotion\" + 0.008*\"mile\" + 0.008*\"td\" + 0.007*\"requirement\" + 0.006*\"offer\" + 0.005*\"citibank\" + 0.005*\"assignment\" + 0.005*\"citi\"'),\n",
       " (4,\n",
       "  '0.021*\"seterus\" + 0.015*\"acct\" + 0.014*\"reinserted\" + 0.010*\"greentree\" + 0.009*\"duplicate\" + 0.007*\"navy\" + 0.007*\"ex-husband\" + 0.007*\"toyota\" + 0.007*\"deletion\" + 0.007*\"mastercard\"'),\n",
       " (5,\n",
       "  '0.015*\"debt\" + 0.012*\"call\" + 0.008*\"collection\" + 0.008*\"phone\" + 0.008*\"calling\" + 0.008*\"number\" + 0.007*\"company\" + 0.006*\"bill\" + 0.006*\"owe\" + 0.006*\"called\"'),\n",
       " (6,\n",
       "  '0.010*\"card\" + 0.008*\"bank\" + 0.008*\"account\" + 0.005*\"fee\" + 0.005*\"``\" + 0.005*\"check\" + 0.005*\"charge\" + 0.004*\"payment\" + 0.004*\"credit\" + 0.004*\"money\"')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "\n",
    "# --- Step 2: LDA Model Training ---\n",
    "# num_topics is a key hyperparameter that defines the number of latent topics to discover.\n",
    "num_topics = 7\n",
    "\n",
    "print(f\"\\nTraining LDA model with {num_topics} topics...\")\n",
    "# We use LdaMulticore for parallelized (faster) training.\n",
    "lda_model = LdaMulticore(\n",
    "    corpus = corpus_tfidf,      # The TF-IDF weighted corpus\n",
    "    id2word = dictionary,       # Mapping from word IDs to words\n",
    "    num_topics = num_topics,    # The number of topics to extract\n",
    "    random_state = 100,         # For reproducibility\n",
    "    chunksize = 100,            # Number of documents to be used in each training chunk\n",
    "    passes = 10                 # Number of passes through the corpus during training\n",
    ")\n",
    "print(\"LDA model training complete.\")\n",
    "\n",
    "\n",
    "# --- Step 3: View the Discovered Topics ---\n",
    "print(\"\\nDiscovered Topics (word distributions):\")\n",
    "# The print_topics() method shows the most influential words for each topic.\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb753f",
   "metadata": {},
   "source": [
    "Assigning the dominant topic (topic with the highest probability for that document) and its human readable label (human interpretation of the separated key words from LDA output) to every complaint in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19112b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic assignment complete. Here is the final DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>topic_probability</th>\n",
       "      <th>topic_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Caliber Home Loans has engaged in the prohibit...</td>\n",
       "      <td>No</td>\n",
       "      <td>[caliber, home, loan, engaged, prohibited, pat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909701</td>\n",
       "      <td>Mortgage &amp; Loan Servicing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>I have filed numerous complaints in an attempt...</td>\n",
       "      <td>No</td>\n",
       "      <td>[filed, numerous, complaint, attempt, stop, na...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.461293</td>\n",
       "      <td>Mortgage &amp; Loan Servicing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2177</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>To Whom it may concern : Consumer Collection M...</td>\n",
       "      <td>No</td>\n",
       "      <td>[may, concern, consumer, collection, managemen...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.790936</td>\n",
       "      <td>Credit Report Issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>Credit card</td>\n",
       "      <td>I received a letter dated XXXX/XXXX/15 stating...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[received, letter, dated, xxxx/xxxx/15, statin...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.877106</td>\n",
       "      <td>Legal Actions &amp; Disputes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>In 2011 I purchase a new phone at a XXXX store...</td>\n",
       "      <td>No</td>\n",
       "      <td>[2011, purchase, new, phone, xxxx, store, xxxx...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.373701</td>\n",
       "      <td>Legal Actions &amp; Disputes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Product                       Consumer complaint narrative  \\\n",
       "1978         Mortgage  Caliber Home Loans has engaged in the prohibit...   \n",
       "2077         Mortgage  I have filed numerous complaints in an attempt...   \n",
       "2177  Debt collection  To Whom it may concern : Consumer Collection M...   \n",
       "2231      Credit card  I received a letter dated XXXX/XXXX/15 stating...   \n",
       "2500  Debt collection  In 2011 I purchase a new phone at a XXXX store...   \n",
       "\n",
       "     Consumer disputed?                                     cleaned_tokens  \\\n",
       "1978                 No  [caliber, home, loan, engaged, prohibited, pat...   \n",
       "2077                 No  [filed, numerous, complaint, attempt, stop, na...   \n",
       "2177                 No  [may, concern, consumer, collection, managemen...   \n",
       "2231                Yes  [received, letter, dated, xxxx/xxxx/15, statin...   \n",
       "2500                 No  [2011, purchase, new, phone, xxxx, store, xxxx...   \n",
       "\n",
       "      dominant_topic  topic_probability                topic_label  \n",
       "1978               0           0.909701  Mortgage & Loan Servicing  \n",
       "2077               0           0.461293  Mortgage & Loan Servicing  \n",
       "2177               2           0.790936       Credit Report Issues  \n",
       "2231               6           0.877106   Legal Actions & Disputes  \n",
       "2500               6           0.373701   Legal Actions & Disputes  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of our topic labels in order from 0 to 6\n",
    "topic_labels = [\n",
    "    \"Mortgage & Loan Servicing\",        # Topic 0\n",
    "    \"Debt Collection Calls\",            # Topic 1\n",
    "    \"Credit Report Issues\",             # Topic 2\n",
    "    \"Credit Card Rewards & Promotions\", # Topic 3\n",
    "    \"Liens & Disputed Re-insertions\",   # Topic 4\n",
    "    \"General Banking & Card Fees\",      # Topic 5\n",
    "    \"Legal Actions & Disputes\"          # Topic 6\n",
    "]\n",
    "\n",
    "# --- Find the dominant topic for each document ---\n",
    "\n",
    "dominant_topics = []\n",
    "topic_percentages = []\n",
    "\n",
    "for doc_bow in corpus_tfidf:\n",
    "    # Get the topic distribution for the document\n",
    "    topic_distribution = lda_model.get_document_topics(doc_bow, minimum_probability=0.0)\n",
    "    # Find the topic with the highest probability\n",
    "    dominant_topic = max(topic_distribution, key=lambda x: x[1])\n",
    "    \n",
    "    dominant_topics.append(dominant_topic[0])\n",
    "    topic_percentages.append(dominant_topic[1])\n",
    "\n",
    "# --- Add the new information to our DataFrame ---\n",
    "complaints_df['dominant_topic'] = dominant_topics\n",
    "complaints_df['topic_probability'] = topic_percentages\n",
    "# Map the topic number to our human-readable label\n",
    "complaints_df['topic_label'] = complaints_df['dominant_topic'].map(lambda topic_id: topic_labels[topic_id])\n",
    "\n",
    "print(\"Topic assignment complete. Here is the final DataFrame:\")\n",
    "# Display the head with our new columns\n",
    "complaints_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3af484",
   "metadata": {},
   "source": [
    "Binary encoding the customer disputed column and exporting the required columns in dataframe to csv file for hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9663693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for export to R...\n",
      "\n",
      "Data successfully prepared and saved to: statistical_analysis_data.csv\n",
      "Here's a preview of the exported data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>topic_label</th>\n",
       "      <th>disputed_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>0</td>\n",
       "      <td>Mortgage &amp; Loan Servicing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>0</td>\n",
       "      <td>Mortgage &amp; Loan Servicing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2177</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>2</td>\n",
       "      <td>Credit Report Issues</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>Credit card</td>\n",
       "      <td>6</td>\n",
       "      <td>Legal Actions &amp; Disputes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>6</td>\n",
       "      <td>Legal Actions &amp; Disputes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Product  dominant_topic                topic_label  \\\n",
       "1978         Mortgage               0  Mortgage & Loan Servicing   \n",
       "2077         Mortgage               0  Mortgage & Loan Servicing   \n",
       "2177  Debt collection               2       Credit Report Issues   \n",
       "2231      Credit card               6   Legal Actions & Disputes   \n",
       "2500  Debt collection               6   Legal Actions & Disputes   \n",
       "\n",
       "      disputed_binary  \n",
       "1978                0  \n",
       "2077                0  \n",
       "2177                0  \n",
       "2231                1  \n",
       "2500                0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Preparing data for export to R...\")\n",
    "\n",
    "# Step 1: Binary Encoding of the target variable\n",
    "# Creating a new column, mapping 'Yes' to 1 and 'No' to 0.\n",
    "complaints_df['disputed_binary'] = complaints_df['Consumer disputed?'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Step 2: Select the final columns needed for statistical analysis\n",
    "final_columns_for_export = [\n",
    "    'Product', \n",
    "    'dominant_topic', \n",
    "    'topic_label', \n",
    "    'disputed_binary'\n",
    "]\n",
    "statistical_df = complaints_df[final_columns_for_export]\n",
    "\n",
    "# Step 3: Save the final DataFrame to a CSV file\n",
    "output_csv_path = 'statistical_analysis_data.csv'\n",
    "\n",
    "# index=False prevents pandas from writing the row numbers into the file\n",
    "statistical_df.to_csv(output_csv_path, index=False) \n",
    "\n",
    "print(f\"\\nData successfully prepared and saved to: {output_csv_path}\")\n",
    "print(\"Here's a preview of the exported data:\")\n",
    "statistical_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
